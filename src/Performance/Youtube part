install.packages("tuber")
install.packages('https')
library(tuber)
myclientid='24403699099-gvcon4ph9qbvotffogimd6fg932m1t6f.apps.googleusercontent.com'
clientsecret='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'
yt_oauth(myclientid,clientsecret,token="")
#â†‘just log in
#get all comments for the youtube video, maybe need a for loop to get the codes for the videos?
#i'm not pretty sure,anybody can help me?
<<<<<<< HEAD
lastcommentseng1=get_all_comments('1OeC9CGtWcM')

=======
lastcommentseng1=get_all_comments('mfD0FY9oHGc')
a <- list_channel_resources(filter = c(channel_id = "UCbulh9WdLtEXiooRcYK7SWw"), part="contentDetails")

# Uploaded playlists:
playlist_id <- a$items[[1]]$contentDetails$relatedPlaylists$uploads

# Get videos on the playlist
vids <- get_playlist_items(filter= c(playlist_id=playlist_id)) 

# Video ids
vid_ids <- as.vector(vids$contentDetails.videoId)

# Function to scrape stats for all vids
get_all_stats <- function(id) {
  get_stats(id)
} 

# Get stats and convert results to data frame 
res <- lapply(vid_ids, get_all_stats)
res_df <- do.call(rbind, lapply(res, data.frame))

head(res_df)
>>>>>>> parent of 0377b0c (Merge branch 'main' of https://github.com/helenapeters/SMWA)



========================================================================
==================         wordcloudpart      ==========================
========================================================================


if (!require("pacman")) install.packages("pacman") ; require("pacman")
p_load(tidyverse,SnowballC, slam, tm,tictoc,qdap)
p_load(udpipe,textplot,ggraph,tidytext,wordcloud,wordcloud2)

# Load the required libraries
library(tm)
library(SnowballC)

# Create a corpus of documents
docs <- lastcommentseng1

# Create a corpus using the TextDocument function from the tm package
corpus <- Corpus(VectorSource(docs))

# Clean and preprocess the corpus
corpus <- tm_map(corpus, content_transformer(tolower)) # Convert to lowercase
corpus <- tm_map(corpus, removePunctuation) # Remove punctuation
corpus <- tm_map(corpus, removeNumbers) # Remove numbers
corpus <- tm_map(corpus, removeWords, stopwords("english")) # Remove stopwords
corpus <- tm_map(corpus, stemDocument, language = "english") # Stem the words

# Create a document term matrix
dtm <- DocumentTermMatrix(corpus)

# Get the frequency of each term in the corpus
freq <- colSums(as.matrix(dtm))

# Sort the terms by frequency
freq <- sort(freq, decreasing = TRUE)

# Print the top 10 terms
head(freq, 10)

tdm <- TermDocumentMatrix(Corpus(VectorSource(corpus)))
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- tibble(word = names(v),freq=v) #or: data.frame(word = names(v),freq=v) 

#using the dplyr to delete the meaningless names
p_load(dplyr)
library(dplyr)
d <- d %>% filter(!(word %in% c("metallica", "song",'album')))

wordcloud2(d)


