install.packages("tuber")
install.packages('https')
library(tuber)
myclientid='24403699099-gvcon4ph9qbvotffogimd6fg932m1t6f.apps.googleusercontent.com'
clientsecret='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'
yt_oauth(myclientid,clientsecret,token="")
#â†‘just log in
#get all comments for the youtube video, maybe need a for loop to get the codes for the videos?
#i'm not pretty sure,anybody can help me?
<<<<<<< HEAD
lastcommentseng1=get_all_comments('1OeC9CGtWcM')

=======
lastcommentseng1=get_all_comments('mfD0FY9oHGc')
a <- list_channel_resources(filter = c(channel_id = "UCbulh9WdLtEXiooRcYK7SWw"), part="contentDetails")

# Uploaded playlists:
playlist_id <- a$items[[1]]$contentDetails$relatedPlaylists$uploads

# Get videos on the playlist
vids <- get_playlist_items(filter= c(playlist_id=playlist_id)) 

# Video ids
vid_ids <- as.vector(vids$contentDetails.videoId)

# Function to scrape stats for all vids
get_all_stats <- function(id) {
  get_stats(id)
} 

# Get stats and convert results to data frame 
res <- lapply(vid_ids, get_all_stats)
res_df <- do.call(rbind, lapply(res, data.frame))

head(res_df)
>>>>>>> parent of 0377b0c (Merge branch 'main' of https://github.com/helenapeters/SMWA)



========================================================================
==================         wordcloudpart      ==========================
========================================================================


if (!require("pacman")) install.packages("pacman") ; require("pacman")
p_load(tidyverse,SnowballC, slam, tm,tictoc,qdap)
p_load(udpipe,textplot,ggraph,tidytext,wordcloud,wordcloud2)

# Load the required libraries
library(tm)
library(SnowballC)

# Create a corpus of documents
docs <- lastcommentseng1

# Create a corpus using the TextDocument function from the tm package
corpus <- Corpus(VectorSource(docs))

# Clean and preprocess the corpus
corpus <- tm_map(corpus, content_transformer(tolower)) # Convert to lowercase
corpus <- tm_map(corpus, removePunctuation) # Remove punctuation
corpus <- tm_map(corpus, removeNumbers) # Remove numbers
corpus <- tm_map(corpus, removeWords, stopwords("english")) # Remove stopwords
corpus <- tm_map(corpus, stemDocument, language = "english") # Stem the words

# Create a document term matrix
dtm <- DocumentTermMatrix(corpus)

# Get the frequency of each term in the corpus
freq <- colSums(as.matrix(dtm))

# Sort the terms by frequency
freq <- sort(freq, decreasing = TRUE)

# Print the top 10 terms
head(freq, 10)

tdm <- TermDocumentMatrix(Corpus(VectorSource(corpus)))
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- tibble(word = names(v),freq=v) #or: data.frame(word = names(v),freq=v) 

#using the dplyr to delete the meaningless names
p_load(dplyr)
library(dplyr)
d <- d %>% filter(!(word %in% c("metallica", "song",'album')))

wordcloud2(d)

======== wordcloud If Darkness Had A Son ================

library(tm)
library(SnowballC)




# Create a corpus of documents
docs <- IfDarknessHadASon_df

# Create a corpus using the TextDocument function from the tm package
corpus <- Corpus(VectorSource(docs))

# Clean and preprocess the corpus
corpus <- tm_map(corpus, content_transformer(tolower)) # Convert to lowercase
corpus <- tm_map(corpus, removePunctuation) # Remove punctuation
corpus <- tm_map(corpus, removeNumbers) # Remove numbers
corpus <- tm_map(corpus, removeWords, stopwords("english")) # Remove stopwords
corpus <- tm_map(corpus, stemDocument, language = "english") # Stem the words

# Create a document term matrix
dtm <- DocumentTermMatrix(corpus)

# Get the frequency of each term in the corpus
freq <- colSums(as.matrix(dtm))

# Sort the terms by frequency
freq <- sort(freq, decreasing = TRUE)

# Print the top 10 terms
head(freq, 10)
Schrijven naar SMWA Word Analysis

cleanText <- function(text) {
  clean_texts <- text %>%
    str_replace_all("<.*>", "") %>% # remove remainig emojis
    str_replace_all("&amp;", "") %>% # remove &
    str_replace_all("(RT|via)((?:\\b\\W*@\\w+)+)", "") %>% # remove retweet entities
    str_replace_all("@\\w+", "") %>% # remove at people
    #str_replace_all("(?:\\s*#\\w+)+\\s*$", "") %>% #remove hashtags in total
    str_replace_all('#', "") %>% #remove only hashtag 
    str_replace_all("[[:punct:]]", "") %>% # remove punctuation
    str_replace_all("[[:digit:]]", "") %>% # remove digits
    str_replace_all("http\\w+", "") %>% # remove html links
    iconv(from = "latin1", to = "ASCII", sub="") %>% # remove emoji and bizarre signs
    str_replace_all("[ \t]{2,}", " ") %>% # remove unnecessary spaces
    str_replace_all("[ \n]", " ") %>% # remove unnecessary spaces
    str_replace_all("^\\s+|\\s+$", "") %>% # remove unnecessary spaces
    str_squish() %>% #remove remaining whitespace 
    str_to_lower() #set all to lower
  return(clean_texts)
}
# cleaned the text with another method because still too much emoji's and bizarre signs
IfDarknessHadASon_cleanedforwordcloud <- cleanText(IfDarknessHadASon_df)

tdm <- TermDocumentMatrix(Corpus(VectorSource(IfDarknessHadASon_cleanedforwordcloud)))
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- tibble(word = names(v),freq=v) #or: data.frame(word = names(v),freq=v) 

search.string <- "#metallica"
d <- d %>% filter(word != tolower(search.string)) %>% arrange(desc(freq))

options(warn=-1) #turn warnings off
wordcloud(d$word,d$freq)
options(warn=0) #turn warnings back on



